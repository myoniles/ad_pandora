# auction_sim

This is the bread and butter of the code.
Most images/queries are performed here by tweaking the parameters in this file. Given the infrequency of use however, I have not set this up to take an args file or some way to go through batches of environments.

*FID* is the fidelity of the simulation. It controls the number of divisions take place within the range of c as well as how many auctions for a given number $n$ take place.

*stage_1_m* and *stage_2_s* describe the underlying distributions of the simulation. They are both of type `Dist` which is defined the in the `dist.py` file.

The only other really important variables in this file are the *metrics* and *trials* array and dictionary.
The former describes the metrics that will be tested over the auctions and the *trials* describes different families of distributions for the environment.

# c_func

Code meant for graphing the optimal c value for a given metric over many values of n.
Moreover a polynomial is fitted to the points using `scipy`.

The file is set up nearly identically to `auction_sim.py`, though an additional constant `LINSPACE` is included. This is the values that c may take within the simulation.

# dist

This is a collection of wrapper classes for distributions.
All distributions inherit from a class called `Dist` which implements the `mean` and `variance` functions. Moreover all `Dist`s have an `est` which is the estimate of the offer within the box.

Although `Dist` only requires floats in their initializers, all children of `Dist` take a `stage_1_mean` and a `stage_1_std` argument.
Both of these are pointers to other `Dist` objects that are used in the generation of the mean and variance of the box.
All distributions are parameterized such that they are in line with the mean and standard deviation generated by these `Dist` objects.

# metric

Metrics are defined by some test that take place over values of $c$.
All metrics related to `Offers` are deprecated as we are not interested in the second price auction settings proposed in the original paper.
The rest expect to act over `Dist` items in the `offers` argument for tests.

Here is a brief description of the metrics contained within this file:

- Revenue Performance:
	- A ratio of the actual profit divided to maximal profit
- Revenue Impact:
	- This is the original measure used in the Bax paper it is 1-revenue performance
- Selectivity:
	- Measures how often, as a percentage, that the selected offer is better than the expected normal max of a normal distribution (like *stage_1_m*) or failing that, the best mean on offer.
- Favorability_GivenTie:
	- Measures how often, as a percentage, that the selected offer is better than the output of alg0 given that algc produced a different outcome
- Favorability_GivenTie_AllTies:
	- Measures how often, as a percentage, that every offer that has a better weighted estimate is better than the output of alg0 given that algc produced a different outcome
- MeanDiff:
	- This is independent from *c*, it is meant to be probed over *n* not for values of *c*. Probe in this fashion using the metric `update=False` argument in `test`.
- \*_strat2:
	- This describes the metrics as described above but uses a failed algorithm for choosing a weighted estimate.

# util

Most functions are helper function to facilitate the simulation originally proposed in the original paper.
There is one exception that being `get_expected_normal_max` which generates the excepted max of a series of normal random variables.
For speed up, a dictionary is used to cache previously calculated values.

# offer

All Objects related to `Offers` are deprecated as we are not interested in the second price auction settings proposed in the original paper.

